{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOC-XEjtg1Nb",
        "outputId": "aeea6c31-c866-4b7c-c2b0-209a2654d978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All dependencies installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# COMPLETE SHOPIFY INSIGHTS-FETCHER WITH COMPETITOR ANALYSIS & DATABASE\n",
        "# ============================================================================\n",
        "\n",
        "# Install all required packages\n",
        "!pip install -q sqlalchemy aiohttp beautifulsoup4 pydantic pandas numpy requests nest-asyncio lxml\n",
        "\n",
        "# Import all necessary libraries\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional, Union\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import nest_asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "from pydantic import BaseModel, validator\n",
        "import sqlite3\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# SQLAlchemy imports\n",
        "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Float, Boolean, ForeignKey, Table, func\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker, relationship\n",
        "\n",
        "# Enable nested event loops for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"âœ… All dependencies installed and imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATABASE SCHEMA AND PYDANTIC MODELS\n",
        "# ============================================================================\n",
        "\n",
        "# SQLAlchemy Base\n",
        "Base = declarative_base()\n",
        "\n",
        "# Association table for competitor relationships\n",
        "competitor_association = Table(\n",
        "    'competitor_relationships',\n",
        "    Base.metadata,\n",
        "    Column('store_id', Integer, ForeignKey('stores.id'), primary_key=True),\n",
        "    Column('competitor_id', Integer, ForeignKey('stores.id'), primary_key=True),\n",
        "    Column('relationship_type', String(50), default='competitor'),\n",
        "    Column('discovered_at', DateTime, default=datetime.utcnow)\n",
        ")\n",
        "\n",
        "# Database Models\n",
        "class StoreDB(Base):\n",
        "    __tablename__ = \"stores\"\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    store_url = Column(String(500), unique=True, nullable=False)\n",
        "    store_name = Column(String(255))\n",
        "    domain = Column(String(255))\n",
        "    industry = Column(String(255))\n",
        "    privacy_policy = Column(Text)\n",
        "    return_policy = Column(Text)\n",
        "    refund_policy = Column(Text)\n",
        "    about_brand = Column(Text)\n",
        "    scraped_at = Column(DateTime, default=datetime.utcnow)\n",
        "    is_competitor = Column(Boolean, default=False)\n",
        "    competitor_discovery_method = Column(String(255))\n",
        "\n",
        "    # Relationships\n",
        "    products = relationship('ProductDB', backref='store', cascade=\"all, delete-orphan\")\n",
        "    faqs = relationship('FAQDB', backref='store', cascade=\"all, delete-orphan\")\n",
        "    social_handles = relationship('SocialHandleDB', backref='store', cascade=\"all, delete-orphan\")\n",
        "    important_links = relationship('ImportantLinkDB', backref='store', cascade=\"all, delete-orphan\")\n",
        "    contact_info = relationship('ContactInfoDB', uselist=False, backref='store', cascade=\"all, delete-orphan\")\n",
        "\n",
        "    # Competitor relationships\n",
        "    competitors = relationship(\n",
        "        'StoreDB',\n",
        "        secondary=competitor_association,\n",
        "        primaryjoin=id == competitor_association.c.store_id,\n",
        "        secondaryjoin=id == competitor_association.c.competitor_id,\n",
        "        backref='competing_with'\n",
        "    )\n",
        "\n",
        "class ProductDB(Base):\n",
        "    __tablename__ = \"products\"\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    product_id = Column(Integer, index=True)\n",
        "    title = Column(String(255))\n",
        "    handle = Column(String(255))\n",
        "    description = Column(Text)\n",
        "    vendor = Column(String(255))\n",
        "    product_type = Column(String(255))\n",
        "    price = Column(Float)\n",
        "    compare_at_price = Column(Float)\n",
        "    available = Column(Boolean)\n",
        "    tags = Column(Text)  # JSON\n",
        "    images = Column(Text)  # JSON\n",
        "    store_id = Column(Integer, ForeignKey('stores.id'))\n",
        "\n",
        "class FAQDB(Base):\n",
        "    __tablename__ = \"faqs\"\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    question = Column(Text)\n",
        "    answer = Column(Text)\n",
        "    category = Column(String(255))\n",
        "    store_id = Column(Integer, ForeignKey('stores.id'))\n",
        "\n",
        "class SocialHandleDB(Base):\n",
        "    __tablename__ = \"social_handles\"\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    platform = Column(String(255))\n",
        "    url = Column(String(512))\n",
        "    username = Column(String(255))\n",
        "    store_id = Column(Integer, ForeignKey('stores.id'))\n",
        "\n",
        "class ImportantLinkDB(Base):\n",
        "    __tablename__ = \"important_links\"\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    name = Column(String(255))\n",
        "    url = Column(String(512))\n",
        "    store_id = Column(Integer, ForeignKey('stores.id'))\n",
        "\n",
        "class ContactInfoDB(Base):\n",
        "    __tablename__ = \"contact_infos\"\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    emails = Column(Text)  # JSON\n",
        "    phone_numbers = Column(Text)  # JSON\n",
        "    address = Column(Text)\n",
        "    store_id = Column(Integer, ForeignKey('stores.id'))\n",
        "\n",
        "class CompetitorAnalysisDB(Base):\n",
        "    __tablename__ = \"competitor_analysis\"\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    original_store_id = Column(Integer, ForeignKey('stores.id'))\n",
        "    analysis_date = Column(DateTime, default=datetime.utcnow)\n",
        "    total_competitors_found = Column(Integer, default=0)\n",
        "    analysis_method = Column(String(255))\n",
        "    analysis_summary = Column(Text)\n",
        "\n",
        "    original_store = relationship('StoreDB', foreign_keys=[original_store_id])\n",
        "\n",
        "# Pydantic Models\n",
        "class Product(BaseModel):\n",
        "    id: int = 0\n",
        "    title: str = \"\"\n",
        "    handle: str = \"\"\n",
        "    description: str = \"\"\n",
        "    vendor: str = \"\"\n",
        "    product_type: str = \"\"\n",
        "    price: float = 0.0\n",
        "    compare_at_price: Optional[float] = None\n",
        "    available: bool = True\n",
        "    tags: List[str] = []\n",
        "    images: List[str] = []\n",
        "\n",
        "    @validator('tags', pre=True)\n",
        "    def parse_tags(cls, v):\n",
        "        if isinstance(v, str):\n",
        "            return [tag.strip() for tag in v.split(',') if tag.strip()]\n",
        "        elif isinstance(v, list):\n",
        "            return [str(tag).strip() for tag in v if tag]\n",
        "        return []\n",
        "\n",
        "    @validator('price', pre=True)\n",
        "    def parse_price(cls, v):\n",
        "        try:\n",
        "            return float(v) if v else 0.0\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "class SocialHandle(BaseModel):\n",
        "    platform: str\n",
        "    url: str\n",
        "    username: Optional[str] = None\n",
        "\n",
        "class ContactInfo(BaseModel):\n",
        "    emails: List[str] = []\n",
        "    phone_numbers: List[str] = []\n",
        "    address: Optional[str] = None\n",
        "\n",
        "class FAQ(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "    category: Optional[str] = None\n",
        "\n",
        "class BrandInsights(BaseModel):\n",
        "    store_url: str\n",
        "    store_name: str\n",
        "    product_catalog: List[Product] = []\n",
        "    hero_products: List[Product] = []\n",
        "    privacy_policy: Optional[str] = None\n",
        "    return_policy: Optional[str] = None\n",
        "    refund_policy: Optional[str] = None\n",
        "    faqs: List[FAQ] = []\n",
        "    social_handles: List[SocialHandle] = []\n",
        "    contact_info: ContactInfo = ContactInfo()\n",
        "    about_brand: Optional[str] = None\n",
        "    important_links: Dict[str, str] = {}\n",
        "    scraped_at: datetime = datetime.now()\n",
        "    is_competitor: bool = False\n",
        "    competitor_discovery_method: Optional[str] = None\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "print(\"âœ… Database schema and models defined!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fhyOFCLiKTL",
        "outputId": "88e20d8c-1dcf-4e3e-eeb7-4e3eb20dd18b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Database schema and models defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CORE SCRAPING ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "class ShopifyStoreScraper:\n",
        "    \"\"\"Core Shopify store scraper with all data extraction capabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = None\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1'\n",
        "        }\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        connector = aiohttp.TCPConnector(limit=30, limit_per_host=10)\n",
        "        self.session = aiohttp.ClientSession(\n",
        "            headers=self.headers,\n",
        "            timeout=aiohttp.ClientTimeout(total=30),\n",
        "            connector=connector\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def scrape_store(self, store_url: str) -> BrandInsights:\n",
        "        \"\"\"Main scraping orchestrator\"\"\"\n",
        "        print(f\"ðŸ” Analyzing: {store_url}\")\n",
        "\n",
        "        if not store_url.startswith('http'):\n",
        "            store_url = f'https://{store_url}'\n",
        "\n",
        "        insights = BrandInsights(\n",
        "            store_url=store_url,\n",
        "            store_name=self._extract_store_name(store_url)\n",
        "        )\n",
        "\n",
        "        # Execute all scraping tasks concurrently\n",
        "        tasks = [\n",
        "            self._scrape_product_catalog(store_url),\n",
        "            self._scrape_hero_products(store_url),\n",
        "            self._scrape_policies(store_url),\n",
        "            self._scrape_faqs(store_url),\n",
        "            self._scrape_social_handles(store_url),\n",
        "            self._scrape_contact_info(store_url),\n",
        "            self._scrape_about_brand(store_url),\n",
        "            self._scrape_important_links(store_url)\n",
        "        ]\n",
        "\n",
        "        print(\"ðŸ“Š Extracting all data points...\")\n",
        "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "        # Parse results\n",
        "        insights.product_catalog = results[0] if not isinstance(results[0], Exception) else []\n",
        "        insights.hero_products = results[1] if not isinstance(results[1], Exception) else []\n",
        "        policies = results[2] if not isinstance(results[2], Exception) else {}\n",
        "        insights.privacy_policy = policies.get('privacy')\n",
        "        insights.return_policy = policies.get('return')\n",
        "        insights.refund_policy = policies.get('refund')\n",
        "        insights.faqs = results[3] if not isinstance(results[3], Exception) else []\n",
        "        insights.social_handles = results[4] if not isinstance(results[4], Exception) else []\n",
        "        insights.contact_info = results[5] if not isinstance(results[5], Exception) else ContactInfo()\n",
        "        insights.about_brand = results[6] if not isinstance(results[6], Exception) else None\n",
        "        insights.important_links = results[7] if not isinstance(results[7], Exception) else {}\n",
        "\n",
        "        print(\"âœ… Store analysis completed!\")\n",
        "        return insights\n",
        "\n",
        "    def _extract_store_name(self, url: str) -> str:\n",
        "        \"\"\"Extract clean store name from URL\"\"\"\n",
        "        domain = urlparse(url).netloc.replace('www.', '')\n",
        "        return domain.split('.')[0].title()\n",
        "\n",
        "    async def _scrape_product_catalog(self, store_url: str) -> List[Product]:\n",
        "        \"\"\"Scrape complete product catalog\"\"\"\n",
        "        print(\"ðŸ“¦ Fetching product catalog...\")\n",
        "\n",
        "        products = []\n",
        "        page = 1\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                url = f\"{store_url.rstrip('/')}/products.json?page={page}&limit=250\"\n",
        "                async with self.session.get(url) as response:\n",
        "                    if response.status != 200:\n",
        "                        break\n",
        "\n",
        "                    data = await response.json()\n",
        "                    page_products = data.get('products', [])\n",
        "\n",
        "                    if not page_products:\n",
        "                        break\n",
        "\n",
        "                    for product_data in page_products:\n",
        "                        try:\n",
        "                            product = Product(\n",
        "                                id=product_data.get('id', 0),\n",
        "                                title=product_data.get('title', ''),\n",
        "                                handle=product_data.get('handle', ''),\n",
        "                                description=self._clean_html(product_data.get('body_html', '')),\n",
        "                                vendor=product_data.get('vendor', ''),\n",
        "                                product_type=product_data.get('product_type', ''),\n",
        "                                available=product_data.get('available', True),\n",
        "                                tags=product_data.get('tags', []),\n",
        "                                images=[img.get('src', '') for img in product_data.get('images', [])]\n",
        "                            )\n",
        "\n",
        "                            # Extract price from first variant\n",
        "                            variants = product_data.get('variants', [])\n",
        "                            if variants:\n",
        "                                product.price = float(variants[0].get('price', 0))\n",
        "                                if variants[0].get('compare_at_price'):\n",
        "                                    product.compare_at_price = float(variants[0]['compare_at_price'])\n",
        "\n",
        "                            products.append(product)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"âš ï¸ Error parsing product: {e}\")\n",
        "                            continue\n",
        "\n",
        "                    page += 1\n",
        "                    if page > 10:  # Limit to prevent infinite loops\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error fetching products page {page}: {e}\")\n",
        "                break\n",
        "\n",
        "        print(f\"âœ… Found {len(products)} products\")\n",
        "        return products\n",
        "\n",
        "    async def _scrape_hero_products(self, store_url: str) -> List[Product]:\n",
        "        \"\"\"Scrape hero products from homepage\"\"\"\n",
        "        print(\"ðŸ  Identifying hero products...\")\n",
        "\n",
        "        try:\n",
        "            async with self.session.get(store_url) as response:\n",
        "                if response.status != 200:\n",
        "                    return []\n",
        "\n",
        "                html = await response.text()\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                product_links = soup.find_all('a', href=re.compile(r'/products/'))\n",
        "                hero_products = []\n",
        "                seen_handles = set()\n",
        "\n",
        "                for link in product_links[:8]:\n",
        "                    href = link.get('href', '')\n",
        "                    if '/products/' in href:\n",
        "                        handle = href.split('/products/')[-1].split('?')[0]\n",
        "\n",
        "                        if handle and handle not in seen_handles:\n",
        "                            seen_handles.add(handle)\n",
        "                            product_data = await self._fetch_product_by_handle(store_url, handle)\n",
        "                            if product_data:\n",
        "                                hero_products.append(product_data)\n",
        "\n",
        "                print(f\"âœ… Found {len(hero_products)} hero products\")\n",
        "                return hero_products\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error scraping hero products: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def _fetch_product_by_handle(self, store_url: str, handle: str) -> Optional[Product]:\n",
        "        \"\"\"Fetch single product by handle\"\"\"\n",
        "        try:\n",
        "            url = f\"{store_url.rstrip('/')}/products/{handle}.json\"\n",
        "            async with self.session.get(url) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    product_info = data.get('product', {})\n",
        "\n",
        "                    product = Product(\n",
        "                        id=product_info.get('id', 0),\n",
        "                        title=product_info.get('title', ''),\n",
        "                        handle=product_info.get('handle', ''),\n",
        "                        description=self._clean_html(product_info.get('body_html', '')),\n",
        "                        vendor=product_info.get('vendor', ''),\n",
        "                        product_type=product_info.get('product_type', ''),\n",
        "                        available=product_info.get('available', True),\n",
        "                        tags=product_info.get('tags', []),\n",
        "                        images=[img.get('src', '') for img in product_info.get('images', [])]\n",
        "                    )\n",
        "\n",
        "                    variants = product_info.get('variants', [])\n",
        "                    if variants:\n",
        "                        product.price = float(variants[0].get('price', 0))\n",
        "\n",
        "                    return product\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    async def _scrape_policies(self, store_url: str) -> Dict[str, str]:\n",
        "        \"\"\"Scrape privacy, return, and refund policies\"\"\"\n",
        "        print(\"ðŸ“œ Extracting policies...\")\n",
        "\n",
        "        policies = {}\n",
        "        policy_endpoints = {\n",
        "            'privacy': ['/policies/privacy-policy', '/pages/privacy-policy', '/pages/privacy'],\n",
        "            'return': ['/policies/return-policy', '/pages/return-policy', '/pages/returns'],\n",
        "            'refund': ['/policies/refund-policy', '/pages/refund-policy', '/pages/refunds']\n",
        "        }\n",
        "\n",
        "        for policy_type, paths in policy_endpoints.items():\n",
        "            for path in paths:\n",
        "                try:\n",
        "                    url = f\"{store_url.rstrip('/')}{path}\"\n",
        "                    async with self.session.get(url) as response:\n",
        "                        if response.status == 200:\n",
        "                            html = await response.text()\n",
        "                            soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                            for element in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
        "                                element.decompose()\n",
        "\n",
        "                            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|policy'))\n",
        "                            if main_content:\n",
        "                                text = main_content.get_text(separator=' ', strip=True)\n",
        "                            else:\n",
        "                                text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "                            cleaned_text = self._clean_text(text)\n",
        "                            if len(cleaned_text) > 200:\n",
        "                                policies[policy_type] = cleaned_text[:3000]\n",
        "                                break\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        print(f\"âœ… Found {len(policies)} policies\")\n",
        "        return policies\n",
        "\n",
        "    async def _scrape_faqs(self, store_url: str) -> List[FAQ]:\n",
        "        \"\"\"Scrape FAQs from common FAQ pages\"\"\"\n",
        "        print(\"â“ Extracting FAQs...\")\n",
        "\n",
        "        faq_paths = ['/pages/faq', '/pages/faqs', '/faq', '/faqs', '/pages/frequently-asked-questions']\n",
        "\n",
        "        for path in faq_paths:\n",
        "            try:\n",
        "                url = f\"{store_url.rstrip('/')}{path}\"\n",
        "                async with self.session.get(url) as response:\n",
        "                    if response.status == 200:\n",
        "                        html = await response.text()\n",
        "                        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                        faqs = []\n",
        "\n",
        "                        # Method 1: Details/Summary structure\n",
        "                        details_elements = soup.find_all('details')\n",
        "                        for detail in details_elements:\n",
        "                            summary = detail.find('summary')\n",
        "                            if summary:\n",
        "                                question = summary.get_text(strip=True)\n",
        "                                answer = detail.get_text(strip=True).replace(question, '', 1).strip()\n",
        "                                if question and answer:\n",
        "                                    faqs.append(FAQ(question=question, answer=answer))\n",
        "\n",
        "                        # Method 2: Common FAQ patterns\n",
        "                        if not faqs:\n",
        "                            faq_containers = soup.find_all(['div', 'section'], class_=re.compile(r'faq|question|accordion'))\n",
        "                            for container in faq_containers:\n",
        "                                question_elem = container.find(['h3', 'h4', 'h5', 'strong'], string=re.compile(r'\\?'))\n",
        "                                if question_elem:\n",
        "                                    question = question_elem.get_text(strip=True)\n",
        "                                    answer_elem = question_elem.find_next_sibling()\n",
        "                                    if answer_elem:\n",
        "                                        answer = answer_elem.get_text(strip=True)\n",
        "                                        if question and answer:\n",
        "                                            faqs.append(FAQ(question=question, answer=answer))\n",
        "\n",
        "                        if faqs:\n",
        "                            print(f\"âœ… Found {len(faqs)} FAQs\")\n",
        "                            return faqs[:20]\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(\"âš ï¸ No FAQs found\")\n",
        "        return []\n",
        "\n",
        "    async def _scrape_social_handles(self, store_url: str) -> List[SocialHandle]:\n",
        "        \"\"\"Extract social media handles\"\"\"\n",
        "        print(\"ðŸ“± Finding social handles...\")\n",
        "\n",
        "        try:\n",
        "            async with self.session.get(store_url) as response:\n",
        "                if response.status != 200:\n",
        "                    return []\n",
        "\n",
        "                html = await response.text()\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                social_handles = []\n",
        "                social_patterns = {\n",
        "                    'instagram': r'(?:instagram\\.com|instagr\\.am)/([^/?\\s&]+)',\n",
        "                    'facebook': r'(?:facebook\\.com|fb\\.com)/([^/?\\s&]+)',\n",
        "                    'twitter': r'(?:twitter\\.com|x\\.com)/([^/?\\s&]+)',\n",
        "                    'youtube': r'youtube\\.com/(?:channel/|user/|c/)?([^/?\\s&]+)',\n",
        "                    'tiktok': r'tiktok\\.com/@?([^/?\\s&]+)',\n",
        "                    'linkedin': r'linkedin\\.com/(?:company/|in/)?([^/?\\s&]+)',\n",
        "                    'pinterest': r'pinterest\\.com/([^/?\\s&]+)'\n",
        "                }\n",
        "\n",
        "                links = soup.find_all('a', href=True)\n",
        "                found_platforms = set()\n",
        "\n",
        "                for link in links:\n",
        "                    href = link.get('href', '').lower()\n",
        "\n",
        "                    for platform, pattern in social_patterns.items():\n",
        "                        if platform not in found_platforms:\n",
        "                            match = re.search(pattern, href, re.IGNORECASE)\n",
        "                            if match:\n",
        "                                username = match.group(1).strip('/')\n",
        "                                if username and len(username) > 0:\n",
        "                                    social_handles.append(SocialHandle(\n",
        "                                        platform=platform,\n",
        "                                        url=link.get('href'),\n",
        "                                        username=username\n",
        "                                    ))\n",
        "                                    found_platforms.add(platform)\n",
        "\n",
        "                print(f\"âœ… Found {len(social_handles)} social handles\")\n",
        "                return social_handles\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error scraping social handles: {e}\")\n",
        "            return []\n",
        "\n",
        "    async def _scrape_contact_info(self, store_url: str) -> ContactInfo:\n",
        "        \"\"\"Extract contact information\"\"\"\n",
        "        print(\"ðŸ“ž Extracting contact info...\")\n",
        "\n",
        "        contact_info = ContactInfo()\n",
        "\n",
        "        contact_urls = [\n",
        "            f\"{store_url.rstrip('/')}/pages/contact\",\n",
        "            f\"{store_url.rstrip('/')}/contact\",\n",
        "            store_url\n",
        "        ]\n",
        "\n",
        "        for url in contact_urls:\n",
        "            try:\n",
        "                async with self.session.get(url) as response:\n",
        "                    if response.status == 200:\n",
        "                        html = await response.text()\n",
        "\n",
        "                        # Extract emails\n",
        "                        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "                        emails = re.findall(email_pattern, html, re.IGNORECASE)\n",
        "\n",
        "                        # Extract phone numbers\n",
        "                        phone_patterns = [\n",
        "                            r'(\\+?91[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
        "                            r'(\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
        "                            r'(\\+?44[-.\\s]?)?\\(?\\d{3,4}\\)?[-.\\s]?\\d{3,4}[-.\\s]?\\d{4}'\n",
        "                        ]\n",
        "\n",
        "                        phones = []\n",
        "                        for pattern in phone_patterns:\n",
        "                            matches = re.findall(pattern, html)\n",
        "                            phones.extend(matches)\n",
        "\n",
        "                        unique_emails = list(set([email.lower() for email in emails if '@' in email]))\n",
        "                        unique_phones = list(set([phone if isinstance(phone, str) else '-'.join(phone) for phone in phones]))\n",
        "\n",
        "                        if unique_emails or unique_phones:\n",
        "                            contact_info.emails = unique_emails[:3]\n",
        "                            contact_info.phone_numbers = unique_phones[:3]\n",
        "                            break\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"âœ… Found {len(contact_info.emails)} emails, {len(contact_info.phone_numbers)} phones\")\n",
        "        return contact_info\n",
        "\n",
        "    async def _scrape_about_brand(self, store_url: str) -> Optional[str]:\n",
        "        \"\"\"Extract about brand information\"\"\"\n",
        "        print(\"â„¹ï¸ Extracting brand info...\")\n",
        "\n",
        "        about_paths = ['/pages/about', '/pages/about-us', '/pages/our-story', '/about']\n",
        "\n",
        "        for path in about_paths:\n",
        "            try:\n",
        "                url = f\"{store_url.rstrip('/')}{path}\"\n",
        "                async with self.session.get(url) as response:\n",
        "                    if response.status == 200:\n",
        "                        html = await response.text()\n",
        "                        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                        for element in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
        "                            element.decompose()\n",
        "\n",
        "                        main_content = soup.find('main') or soup.find('div', class_=re.compile(r'about|content'))\n",
        "                        if main_content:\n",
        "                            text = main_content.get_text(separator=' ', strip=True)\n",
        "                            cleaned_text = self._clean_text(text)\n",
        "                            if len(cleaned_text) > 100:\n",
        "                                print(\"âœ… Found brand information\")\n",
        "                                return cleaned_text[:2000]\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(\"âš ï¸ No brand info found\")\n",
        "        return None\n",
        "\n",
        "    async def _scrape_important_links(self, store_url: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract important links\"\"\"\n",
        "        print(\"ðŸ”— Finding important links...\")\n",
        "\n",
        "        try:\n",
        "            async with self.session.get(store_url) as response:\n",
        "                if response.status != 200:\n",
        "                    return {}\n",
        "\n",
        "                html = await response.text()\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                important_links = {}\n",
        "                important_keywords = {\n",
        "                    'Order Tracking': ['track', 'tracking', 'order tracking', 'track order'],\n",
        "                    'Contact Us': ['contact', 'contact us', 'get in touch'],\n",
        "                    'Blog': ['blog', 'news', 'articles'],\n",
        "                    'Size Guide': ['size', 'size guide', 'sizing'],\n",
        "                    'Shipping': ['shipping', 'delivery', 'shipping info'],\n",
        "                    'Returns': ['returns', 'return policy'],\n",
        "                    'Support': ['support', 'help', 'customer service']\n",
        "                }\n",
        "\n",
        "                links = soup.find_all('a', href=True)\n",
        "\n",
        "                for link in links:\n",
        "                    href = link.get('href', '')\n",
        "                    text = link.get_text(strip=True).lower()\n",
        "\n",
        "                    if len(text) > 50:\n",
        "                        continue\n",
        "\n",
        "                    for category, keywords in important_keywords.items():\n",
        "                        if any(keyword in text for keyword in keywords):\n",
        "                            if href.startswith('/'):\n",
        "                                href = f\"{store_url.rstrip('/')}{href}\"\n",
        "                            elif not href.startswith('http'):\n",
        "                                href = f\"{store_url.rstrip('/')}/{href}\"\n",
        "\n",
        "                            important_links[category] = href\n",
        "                            break\n",
        "\n",
        "                print(f\"âœ… Found {len(important_links)} important links\")\n",
        "                return important_links\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error scraping important links: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _clean_html(self, html_string: str) -> str:\n",
        "        \"\"\"Clean HTML content\"\"\"\n",
        "        if not html_string:\n",
        "            return \"\"\n",
        "        soup = BeautifulSoup(html_string, 'html.parser')\n",
        "        return soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "print(\"âœ… Core scraping engine ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqQ47GuDidAu",
        "outputId": "33896821-2757-4c8d-f893-f33c954c7103"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Core scraping engine ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPETITOR DISCOVERY ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "class CompetitorDiscoveryEngine:\n",
        "    \"\"\"Advanced competitor discovery using multiple methods\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "    def extract_brand_info(self, store_url: str) -> dict:\n",
        "        \"\"\"Extract brand information for competitor search\"\"\"\n",
        "        try:\n",
        "            response = requests.get(store_url, headers=self.headers, timeout=10)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            brand_name = self._extract_brand_name(store_url, soup)\n",
        "            industry = self._extract_industry(soup)\n",
        "            products = self._extract_primary_products(soup)\n",
        "\n",
        "            return {\n",
        "                'brand_name': brand_name,\n",
        "                'industry': industry,\n",
        "                'products': products,\n",
        "                'domain': urlparse(store_url).netloc\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error extracting brand info: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_brand_name(self, store_url: str, soup: BeautifulSoup) -> str:\n",
        "        \"\"\"Extract brand name from various sources\"\"\"\n",
        "        title = soup.find('title')\n",
        "        if title:\n",
        "            brand_name = title.get_text().split('-')[0].strip()\n",
        "            if brand_name:\n",
        "                return brand_name\n",
        "\n",
        "        domain = urlparse(store_url).netloc.replace('www.', '')\n",
        "        return domain.split('.')[0].title()\n",
        "\n",
        "    def _extract_industry(self, soup: BeautifulSoup) -> str:\n",
        "        \"\"\"Extract industry/category from page content\"\"\"\n",
        "        industry_indicators = [\n",
        "            'fashion', 'clothing', 'apparel', 'beauty', 'cosmetics', 'skincare',\n",
        "            'electronics', 'gadgets', 'tech', 'home', 'decor', 'furniture',\n",
        "            'fitness', 'sports', 'health', 'wellness', 'food', 'beverage'\n",
        "        ]\n",
        "\n",
        "        page_text = soup.get_text().lower()\n",
        "        for indicator in industry_indicators:\n",
        "            if indicator in page_text:\n",
        "                return indicator.title()\n",
        "\n",
        "        return 'General'\n",
        "\n",
        "    def _extract_primary_products(self, soup: BeautifulSoup) -> list:\n",
        "        \"\"\"Extract primary product types\"\"\"\n",
        "        products = []\n",
        "        product_links = soup.find_all('a', href=re.compile(r'/products/'))\n",
        "        for link in product_links[:5]:\n",
        "            product_text = link.get_text().strip()\n",
        "            if product_text and len(product_text) > 3:\n",
        "                products.append(product_text)\n",
        "        return products\n",
        "\n",
        "    def discover_competitors_by_industry(self, brand_info: dict, max_results: int = 5) -> list:\n",
        "        \"\"\"Discover competitors by industry analysis\"\"\"\n",
        "        competitors = []\n",
        "\n",
        "        # Industry-specific competitor databases\n",
        "        industry_competitors = {\n",
        "            'fashion': [\n",
        "                {'url': 'https://www.zara.com', 'name': 'Zara', 'shopify': False},\n",
        "                {'url': 'https://www.hm.com', 'name': 'H&M', 'shopify': False},\n",
        "                {'url': 'https://bombayshirts.com', 'name': 'Bombay Shirts', 'shopify': True},\n",
        "                {'url': 'https://www.koovs.com', 'name': 'Koovs', 'shopify': False},\n",
        "                {'url': 'https://www.thewhitehangers.com', 'name': 'The White Hangers', 'shopify': True}\n",
        "            ],\n",
        "            'beauty': [\n",
        "                {'url': 'https://www.nykaa.com', 'name': 'Nykaa', 'shopify': False},\n",
        "                {'url': 'https://www.purplle.com', 'name': 'Purplle', 'shopify': False},\n",
        "                {'url': 'https://www.mcaffeine.com', 'name': 'mCaffeine', 'shopify': True},\n",
        "                {'url': 'https://www.thebodyshop.in', 'name': 'The Body Shop', 'shopify': False}\n",
        "            ],\n",
        "            'electronics': [\n",
        "                {'url': 'https://www.flipkart.com', 'name': 'Flipkart', 'shopify': False},\n",
        "                {'url': 'https://www.amazon.in', 'name': 'Amazon India', 'shopify': False},\n",
        "                {'url': 'https://www.croma.com', 'name': 'Croma', 'shopify': False}\n",
        "            ],\n",
        "            'general': [\n",
        "                {'url': 'https://hairoriginals.com', 'name': 'Hair Originals', 'shopify': True},\n",
        "                {'url': 'https://www.bewakoof.com', 'name': 'Bewakoof', 'shopify': False},\n",
        "                {'url': 'https://www.myntra.com', 'name': 'Myntra', 'shopify': False}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        industry = brand_info.get('industry', 'general').lower()\n",
        "        industry_stores = industry_competitors.get(industry, industry_competitors['general'])\n",
        "\n",
        "        # Filter for Shopify stores and add to competitors\n",
        "        for store in industry_stores:\n",
        "            if len(competitors) >= max_results:\n",
        "                break\n",
        "\n",
        "            # For demo purposes, we'll include some non-Shopify stores\n",
        "            # In production, you'd filter only Shopify stores\n",
        "            competitors.append({\n",
        "                'url': store['url'],\n",
        "                'name': store['name'],\n",
        "                'discovery_method': 'industry_analysis',\n",
        "                'relationship_type': 'indirect',\n",
        "                'is_shopify': store.get('shopify', False)\n",
        "            })\n",
        "\n",
        "        return competitors[:max_results]\n",
        "\n",
        "    def discover_competitors_by_similarity(self, brand_info: dict, max_results: int = 3) -> list:\n",
        "        \"\"\"Discover competitors by analyzing similar characteristics\"\"\"\n",
        "        competitors = []\n",
        "\n",
        "        # Simulated competitor discovery based on brand characteristics\n",
        "        similar_stores = [\n",
        "            {'url': 'https://www.ajio.com', 'name': 'Ajio', 'discovery_method': 'similarity_analysis'},\n",
        "            {'url': 'https://www.jabong.com', 'name': 'Jabong', 'discovery_method': 'similarity_analysis'},\n",
        "            {'url': 'https://www.limeroad.com', 'name': 'Limeroad', 'discovery_method': 'similarity_analysis'}\n",
        "        ]\n",
        "\n",
        "        for store in similar_stores[:max_results]:\n",
        "            competitors.append({\n",
        "                'url': store['url'],\n",
        "                'name': store['name'],\n",
        "                'discovery_method': store['discovery_method'],\n",
        "                'relationship_type': 'potential'\n",
        "            })\n",
        "\n",
        "        return competitors\n",
        "\n",
        "print(\"âœ… Competitor discovery engine ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7eQkmdmidDG",
        "outputId": "20e29ef5-1f6d-42b6-cd24-1fe79b4a7777"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Competitor discovery engine ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATABASE MANAGER\n",
        "# ============================================================================\n",
        "\n",
        "class DatabaseManager:\n",
        "    \"\"\"Handles all database operations\"\"\"\n",
        "\n",
        "    def __init__(self, db_url=\"sqlite:///shopify_competitor_insights.db\"):\n",
        "        self.engine = create_engine(db_url, echo=False)\n",
        "        Base.metadata.create_all(self.engine)\n",
        "        self.Session = sessionmaker(bind=self.engine)\n",
        "        print(f\"âœ… Database initialized: {db_url}\")\n",
        "\n",
        "    def save_store_insights(self, insights: BrandInsights, brand_info: dict = None) -> int:\n",
        "        \"\"\"Save store insights to database\"\"\"\n",
        "        session = self.Session()\n",
        "\n",
        "        try:\n",
        "            # Check if store exists\n",
        "            existing_store = session.query(StoreDB).filter_by(store_url=insights.store_url).first()\n",
        "            if existing_store:\n",
        "                session.delete(existing_store)\n",
        "                session.commit()\n",
        "\n",
        "            # Create new store record\n",
        "            store = StoreDB(\n",
        "                store_url=insights.store_url,\n",
        "                store_name=insights.store_name,\n",
        "                domain=urlparse(insights.store_url).netloc,\n",
        "                industry=brand_info.get('industry', 'Unknown') if brand_info else 'Unknown',\n",
        "                privacy_policy=insights.privacy_policy,\n",
        "                return_policy=insights.return_policy,\n",
        "                refund_policy=insights.refund_policy,\n",
        "                about_brand=insights.about_brand,\n",
        "                scraped_at=insights.scraped_at,\n",
        "                is_competitor=insights.is_competitor,\n",
        "                competitor_discovery_method=insights.competitor_discovery_method\n",
        "            )\n",
        "\n",
        "            # Add contact info\n",
        "            if insights.contact_info:\n",
        "                contact_info = ContactInfoDB(\n",
        "                    emails=json.dumps(insights.contact_info.emails),\n",
        "                    phone_numbers=json.dumps(insights.contact_info.phone_numbers),\n",
        "                    address=insights.contact_info.address,\n",
        "                    store=store\n",
        "                )\n",
        "                session.add(contact_info)\n",
        "\n",
        "            # Add products\n",
        "            for product in insights.product_catalog:\n",
        "                product_db = ProductDB(\n",
        "                    product_id=product.id,\n",
        "                    title=product.title,\n",
        "                    handle=product.handle,\n",
        "                    description=product.description,\n",
        "                    vendor=product.vendor,\n",
        "                    product_type=product.product_type,\n",
        "                    price=product.price,\n",
        "                    compare_at_price=product.compare_at_price,\n",
        "                    available=product.available,\n",
        "                    tags=json.dumps(product.tags),\n",
        "                    images=json.dumps(product.images),\n",
        "                    store=store\n",
        "                )\n",
        "                session.add(product_db)\n",
        "\n",
        "            # Add FAQs\n",
        "            for faq in insights.faqs:\n",
        "                faq_db = FAQDB(\n",
        "                    question=faq.question,\n",
        "                    answer=faq.answer,\n",
        "                    category=getattr(faq, 'category', None),\n",
        "                    store=store\n",
        "                )\n",
        "                session.add(faq_db)\n",
        "\n",
        "            # Add social handles\n",
        "            for handle in insights.social_handles:\n",
        "                handle_db = SocialHandleDB(\n",
        "                    platform=handle.platform,\n",
        "                    url=handle.url,\n",
        "                    username=handle.username,\n",
        "                    store=store\n",
        "                )\n",
        "                session.add(handle_db)\n",
        "\n",
        "            # Add important links\n",
        "            for name, url in insights.important_links.items():\n",
        "                link_db = ImportantLinkDB(\n",
        "                    name=name,\n",
        "                    url=url,\n",
        "                    store=store\n",
        "                )\n",
        "                session.add(link_db)\n",
        "\n",
        "            session.add(store)\n",
        "            session.commit()\n",
        "\n",
        "            store_id = store.id\n",
        "            print(f\"âœ… Store saved to database with ID: {store_id}\")\n",
        "            return store_id\n",
        "\n",
        "        except Exception as e:\n",
        "            session.rollback()\n",
        "            print(f\"âŒ Error saving store: {e}\")\n",
        "            raise\n",
        "        finally:\n",
        "            session.close()\n",
        "\n",
        "    def create_competitor_relationship(self, original_store_id: int, competitor_store_id: int, relationship_type: str = 'competitor'):\n",
        "        \"\"\"Create competitor relationship\"\"\"\n",
        "        session = self.Session()\n",
        "\n",
        "        try:\n",
        "            # Check if relationship already exists\n",
        "            existing = session.query(competitor_association).filter_by(\n",
        "                store_id=original_store_id,\n",
        "                competitor_id=competitor_store_id\n",
        "            ).first()\n",
        "\n",
        "            if not existing:\n",
        "                session.execute(\n",
        "                    competitor_association.insert().values(\n",
        "                        store_id=original_store_id,\n",
        "                        competitor_id=competitor_store_id,\n",
        "                        relationship_type=relationship_type,\n",
        "                        discovered_at=datetime.utcnow()\n",
        "                    )\n",
        "                )\n",
        "                session.commit()\n",
        "                print(f\"âœ… Competitor relationship created: {original_store_id} -> {competitor_store_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            session.rollback()\n",
        "            print(f\"âŒ Error creating competitor relationship: {e}\")\n",
        "        finally:\n",
        "            session.close()\n",
        "\n",
        "    def get_store_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Get summary of all stores\"\"\"\n",
        "        session = self.Session()\n",
        "\n",
        "        try:\n",
        "            stores = session.query(StoreDB).all()\n",
        "            data = []\n",
        "\n",
        "            for store in stores:\n",
        "                product_count = session.query(ProductDB).filter_by(store_id=store.id).count()\n",
        "                social_count = session.query(SocialHandleDB).filter_by(store_id=store.id).count()\n",
        "                faq_count = session.query(FAQDB).filter_by(store_id=store.id).count()\n",
        "\n",
        "                data.append({\n",
        "                    'Store ID': store.id,\n",
        "                    'Store Name': store.store_name,\n",
        "                    'Industry': store.industry,\n",
        "                    'Products': product_count,\n",
        "                    'Social Platforms': social_count,\n",
        "                    'FAQs': faq_count,\n",
        "                    'Is Competitor': store.is_competitor,\n",
        "                    'Scraped At': store.scraped_at.strftime('%Y-%m-%d %H:%M')\n",
        "                })\n",
        "\n",
        "            return pd.DataFrame(data)\n",
        "\n",
        "        finally:\n",
        "            session.close()\n",
        "\n",
        "    def generate_competitive_report(self, store_id: int) -> dict:\n",
        "        \"\"\"Generate competitive analysis report\"\"\"\n",
        "        session = self.Session()\n",
        "\n",
        "        try:\n",
        "            original_store = session.query(StoreDB).filter_by(id=store_id).first()\n",
        "            if not original_store:\n",
        "                return {'error': f'Store with ID {store_id} not found'}\n",
        "\n",
        "            # Get competitors\n",
        "            competitors = session.query(StoreDB).join(\n",
        "                competitor_association,\n",
        "                StoreDB.id == competitor_association.c.competitor_id\n",
        "            ).filter(competitor_association.c.store_id == store_id).all()\n",
        "\n",
        "            # Calculate metrics\n",
        "            original_metrics = self._calculate_store_metrics(session, original_store)\n",
        "            competitor_metrics = [self._calculate_store_metrics(session, comp) for comp in competitors]\n",
        "\n",
        "            # Generate insights\n",
        "            insights = self._generate_competitive_insights(original_metrics, competitor_metrics)\n",
        "\n",
        "            report = {\n",
        "                'original_store': original_metrics,\n",
        "                'competitors': competitor_metrics,\n",
        "                'competitive_insights': insights,\n",
        "                'generated_at': datetime.utcnow().isoformat()\n",
        "            }\n",
        "\n",
        "            return report\n",
        "\n",
        "        finally:\n",
        "            session.close()\n",
        "\n",
        "    def _calculate_store_metrics(self, session, store: StoreDB) -> dict:\n",
        "        \"\"\"Calculate metrics for a store\"\"\"\n",
        "        product_count = session.query(ProductDB).filter_by(store_id=store.id).count()\n",
        "\n",
        "        price_stats = session.query(\n",
        "            func.avg(ProductDB.price).label('avg_price'),\n",
        "            func.min(ProductDB.price).label('min_price'),\n",
        "            func.max(ProductDB.price).label('max_price')\n",
        "        ).filter_by(store_id=store.id).first()\n",
        "\n",
        "        social_count = session.query(SocialHandleDB).filter_by(store_id=store.id).count()\n",
        "        faq_count = session.query(FAQDB).filter_by(store_id=store.id).count()\n",
        "\n",
        "        return {\n",
        "            'store_name': store.store_name,\n",
        "            'store_url': store.store_url,\n",
        "            'industry': store.industry,\n",
        "            'total_products': product_count,\n",
        "            'price_stats': {\n",
        "                'average_price': float(price_stats.avg_price) if price_stats.avg_price else 0,\n",
        "                'min_price': float(price_stats.min_price) if price_stats.min_price else 0,\n",
        "                'max_price': float(price_stats.max_price) if price_stats.max_price else 0\n",
        "            },\n",
        "            'social_platforms': social_count,\n",
        "            'faq_count': faq_count,\n",
        "            'has_privacy_policy': bool(store.privacy_policy),\n",
        "            'has_return_policy': bool(store.return_policy),\n",
        "            'has_about_section': bool(store.about_brand)\n",
        "        }\n",
        "\n",
        "    def _generate_competitive_insights(self, original_metrics: dict, competitor_metrics: list) -> dict:\n",
        "        \"\"\"Generate competitive insights\"\"\"\n",
        "        if not competitor_metrics:\n",
        "            return {'message': 'No competitors found for analysis'}\n",
        "\n",
        "        # Product comparison\n",
        "        original_products = original_metrics['total_products']\n",
        "        competitor_products = [comp['total_products'] for comp in competitor_metrics]\n",
        "        avg_competitor_products = sum(competitor_products) / len(competitor_products) if competitor_products else 0\n",
        "\n",
        "        # Price comparison\n",
        "        original_avg_price = original_metrics['price_stats']['average_price']\n",
        "        competitor_avg_prices = [comp['price_stats']['average_price'] for comp in competitor_metrics]\n",
        "        avg_competitor_price = sum(competitor_avg_prices) / len(competitor_avg_prices) if competitor_avg_prices else 0\n",
        "\n",
        "        insights = {\n",
        "            'product_catalog_comparison': {\n",
        "                'original_store_products': original_products,\n",
        "                'competitor_average': avg_competitor_products,\n",
        "                'position': 'above_average' if original_products > avg_competitor_products else 'below_average'\n",
        "            },\n",
        "            'pricing_comparison': {\n",
        "                'original_avg_price': original_avg_price,\n",
        "                'competitor_avg_price': avg_competitor_price,\n",
        "                'pricing_position': 'premium' if original_avg_price > avg_competitor_price else 'competitive'\n",
        "            },\n",
        "            'feature_analysis': {\n",
        "                'original_social_platforms': original_metrics['social_platforms'],\n",
        "                'competitor_avg_social': sum([comp['social_platforms'] for comp in competitor_metrics]) / len(competitor_metrics),\n",
        "                'original_faq_count': original_metrics['faq_count'],\n",
        "                'competitor_avg_faq': sum([comp['faq_count'] for comp in competitor_metrics]) / len(competitor_metrics)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return insights\n",
        "\n",
        "print(\"âœ… Database manager ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daa1dLI3idFl",
        "outputId": "93c85d00-f899-4381-f333-d1f483e7a318"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Database manager ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN APPLICATION - COMPLETE SHOPIFY COMPETITOR ANALYZER\n",
        "# ============================================================================\n",
        "\n",
        "class CompleteShopifyAnalyzer:\n",
        "    \"\"\"Complete Shopify analyzer with competitor analysis and database persistence\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scraper = ShopifyStoreScraper()\n",
        "        self.competitor_engine = CompetitorDiscoveryEngine()\n",
        "        self.db_manager = DatabaseManager()\n",
        "\n",
        "    async def comprehensive_analysis(self, store_url: str, include_competitors: bool = True, max_competitors: int = 3) -> dict:\n",
        "        \"\"\"Run complete analysis with competitor discovery and database persistence\"\"\"\n",
        "        print(f\"ðŸš€ Starting comprehensive analysis for: {store_url}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Step 1: Analyze original store\n",
        "        print(\"ðŸ“Š Analyzing original store...\")\n",
        "        async with self.scraper as scraper:\n",
        "            original_insights = await scraper.scrape_store(store_url)\n",
        "\n",
        "        # Step 2: Extract brand information\n",
        "        print(\"ðŸ” Extracting brand information...\")\n",
        "        brand_info = self.competitor_engine.extract_brand_info(store_url)\n",
        "\n",
        "        # Step 3: Save original store to database\n",
        "        print(\"ðŸ’¾ Saving original store to database...\")\n",
        "        original_store_id = self.db_manager.save_store_insights(original_insights, brand_info)\n",
        "\n",
        "        competitor_insights = []\n",
        "        competitor_store_ids = []\n",
        "\n",
        "        if include_competitors:\n",
        "            # Step 4: Discover competitors\n",
        "            print(\"ðŸŽ¯ Discovering competitors...\")\n",
        "            industry_competitors = self.competitor_engine.discover_competitors_by_industry(brand_info, max_competitors)\n",
        "            similarity_competitors = self.competitor_engine.discover_competitors_by_similarity(brand_info, max_competitors)\n",
        "\n",
        "            all_competitors = industry_competitors + similarity_competitors\n",
        "            unique_competitors = self._deduplicate_competitors(all_competitors)[:max_competitors]\n",
        "\n",
        "            print(f\"Found {len(unique_competitors)} unique competitors\")\n",
        "\n",
        "            # Step 5: Analyze competitor stores\n",
        "            print(\"ðŸ“ˆ Analyzing competitor stores...\")\n",
        "            for i, competitor in enumerate(unique_competitors, 1):\n",
        "                try:\n",
        "                    print(f\"  [{i}/{len(unique_competitors)}] Analyzing: {competitor['name']}\")\n",
        "\n",
        "                    # Only analyze if it's a potential Shopify store\n",
        "                    if self._is_likely_shopify(competitor['url']):\n",
        "                        async with self.scraper as scraper:\n",
        "                            competitor_insight = await scraper.scrape_store(competitor['url'])\n",
        "\n",
        "                        competitor_insight.is_competitor = True\n",
        "                        competitor_insight.competitor_discovery_method = competitor['discovery_method']\n",
        "\n",
        "                        # Save competitor to database\n",
        "                        competitor_store_id = self.db_manager.save_store_insights(competitor_insight, brand_info)\n",
        "\n",
        "                        # Create competitor relationship\n",
        "                        self.db_manager.create_competitor_relationship(\n",
        "                            original_store_id,\n",
        "                            competitor_store_id,\n",
        "                            competitor['relationship_type']\n",
        "                        )\n",
        "\n",
        "                        competitor_insights.append(competitor_insight)\n",
        "                        competitor_store_ids.append(competitor_store_id)\n",
        "\n",
        "                    else:\n",
        "                        print(f\"    âš ï¸ Skipping {competitor['name']} - Not a Shopify store\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    âŒ Failed to analyze {competitor['name']}: {e}\")\n",
        "\n",
        "        # Step 6: Generate comprehensive report\n",
        "        print(\"ðŸ“‹ Generating comprehensive report...\")\n",
        "        competitive_report = self.db_manager.generate_competitive_report(original_store_id)\n",
        "\n",
        "        # Step 7: Display results\n",
        "        self._display_comprehensive_results(original_insights, competitor_insights, competitive_report)\n",
        "\n",
        "        # Step 8: Export data\n",
        "        self._export_analysis_data(original_insights, competitor_insights, competitive_report)\n",
        "\n",
        "        print(\"âœ… Comprehensive analysis completed!\")\n",
        "\n",
        "        return {\n",
        "            'original_store': original_insights,\n",
        "            'competitors': competitor_insights,\n",
        "            'brand_info': brand_info,\n",
        "            'competitive_report': competitive_report,\n",
        "            'original_store_id': original_store_id,\n",
        "            'competitor_store_ids': competitor_store_ids\n",
        "        }\n",
        "\n",
        "    def _deduplicate_competitors(self, competitors: list) -> list:\n",
        "        \"\"\"Remove duplicate competitors\"\"\"\n",
        "        seen_urls = set()\n",
        "        unique_competitors = []\n",
        "\n",
        "        for competitor in competitors:\n",
        "            if competitor['url'] not in seen_urls:\n",
        "                unique_competitors.append(competitor)\n",
        "                seen_urls.add(competitor['url'])\n",
        "\n",
        "        return unique_competitors\n",
        "\n",
        "    def _is_likely_shopify(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL is likely a Shopify store\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
        "            content = response.text.lower()\n",
        "\n",
        "            shopify_indicators = [\n",
        "                'shopify',\n",
        "                'cdn.shopify.com',\n",
        "                'shopifycdn.com',\n",
        "                'myshopify.com'\n",
        "            ]\n",
        "\n",
        "            return any(indicator in content for indicator in shopify_indicators)\n",
        "\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def _display_comprehensive_results(self, original_insights: BrandInsights, competitor_insights: list, competitive_report: dict):\n",
        "        \"\"\"Display comprehensive analysis results\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ“Š COMPREHENSIVE ANALYSIS RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Original store summary\n",
        "        print(f\"\\nðŸª ORIGINAL STORE: {original_insights.store_name}\")\n",
        "        print(f\"URL: {original_insights.store_url}\")\n",
        "        print(f\"Products: {len(original_insights.product_catalog)}\")\n",
        "        print(f\"Hero Products: {len(original_insights.hero_products)}\")\n",
        "        print(f\"Social Platforms: {len(original_insights.social_handles)}\")\n",
        "        print(f\"FAQs: {len(original_insights.faqs)}\")\n",
        "        print(f\"Contact Emails: {len(original_insights.contact_info.emails)}\")\n",
        "        print(f\"Contact Phones: {len(original_insights.contact_info.phone_numbers)}\")\n",
        "\n",
        "        # Competitor summary\n",
        "        print(f\"\\nðŸ† COMPETITORS ANALYZED: {len(competitor_insights)}\")\n",
        "        for i, competitor in enumerate(competitor_insights, 1):\n",
        "            print(f\"  {i}. {competitor.store_name}\")\n",
        "            print(f\"     URL: {competitor.store_url}\")\n",
        "            print(f\"     Products: {len(competitor.product_catalog)}\")\n",
        "            print(f\"     Discovery Method: {competitor.competitor_discovery_method}\")\n",
        "\n",
        "        # Competitive insights\n",
        "        if 'competitive_insights' in competitive_report and competitive_report['competitive_insights']:\n",
        "            insights = competitive_report['competitive_insights']\n",
        "            print(f\"\\nðŸ” COMPETITIVE INSIGHTS:\")\n",
        "            print(f\"Product Catalog Position: {insights['product_catalog_comparison']['position']}\")\n",
        "            print(f\"Pricing Position: {insights['pricing_comparison']['pricing_position']}\")\n",
        "            print(f\"Average Price: ${insights['pricing_comparison']['original_avg_price']:.2f}\")\n",
        "            print(f\"Competitor Average Price: ${insights['pricing_comparison']['competitor_avg_price']:.2f}\")\n",
        "\n",
        "        # Display key data points\n",
        "        print(f\"\\nðŸ“¦ PRODUCT INSIGHTS:\")\n",
        "        if original_insights.product_catalog:\n",
        "            print(\"Top Products:\")\n",
        "            for i, product in enumerate(original_insights.product_catalog[:5], 1):\n",
        "                print(f\"  {i}. {product.title} - ${product.price}\")\n",
        "\n",
        "        print(f\"\\nðŸ“± SOCIAL MEDIA PRESENCE:\")\n",
        "        for handle in original_insights.social_handles:\n",
        "            print(f\"  {handle.platform.title()}: {handle.url}\")\n",
        "\n",
        "        print(f\"\\nâ“ CUSTOMER SUPPORT:\")\n",
        "        if original_insights.faqs:\n",
        "            print(\"Sample FAQs:\")\n",
        "            for i, faq in enumerate(original_insights.faqs[:3], 1):\n",
        "                print(f\"  Q{i}: {faq.question}\")\n",
        "                print(f\"  A{i}: {faq.answer[:100]}...\")\n",
        "\n",
        "        print(f\"\\nðŸ”— IMPORTANT LINKS:\")\n",
        "        for name, url in original_insights.important_links.items():\n",
        "            print(f\"  {name}: {url}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    def _export_analysis_data(self, original_insights: BrandInsights, competitor_insights: list, competitive_report: dict):\n",
        "        \"\"\"Export analysis data to files\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Export original store data\n",
        "        original_filename = f\"{original_insights.store_name.lower().replace(' ', '_')}_analysis_{timestamp}.json\"\n",
        "        with open(original_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(original_insights.dict(), f, indent=2, default=str, ensure_ascii=False)\n",
        "\n",
        "        # Export competitive report\n",
        "        report_filename = f\"{original_insights.store_name.lower().replace(' ', '_')}_competitive_report_{timestamp}.json\"\n",
        "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(competitive_report, f, indent=2, default=str, ensure_ascii=False)\n",
        "\n",
        "        print(f\"ðŸ“ Analysis exported to: {original_filename}\")\n",
        "        print(f\"ðŸ“ Competitive report exported to: {report_filename}\")\n",
        "\n",
        "    def get_database_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Get summary of all data in database\"\"\"\n",
        "        return self.db_manager.get_store_summary()\n",
        "\n",
        "print(\"âœ… Complete Shopify analyzer ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z7CoyjHidH8",
        "outputId": "5fd0e12e-0446-4203-cd65-aacf8b25e91d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Complete Shopify analyzer ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vnXMiYzjiKgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DEMO EXECUTION - RUN THE COMPLETE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "async def run_complete_demo():\n",
        "    \"\"\"Run the complete demonstration\"\"\"\n",
        "    print(\"ðŸŽ¯ COMPLETE SHOPIFY COMPETITOR ANALYSIS DEMO\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = CompleteShopifyAnalyzer()\n",
        "\n",
        "    # Target store for analysis\n",
        "    target_store = \"https://memy.co.in\"\n",
        "\n",
        "    # Run comprehensive analysis\n",
        "    results = await analyzer.comprehensive_analysis(\n",
        "        store_url=target_store,\n",
        "        include_competitors=True,\n",
        "        max_competitors=2  # Limit for demo\n",
        "    )\n",
        "\n",
        "    print(f\"\\nðŸ“Š ANALYSIS SUMMARY:\")\n",
        "    print(f\"Original Store: {results['original_store'].store_name}\")\n",
        "    print(f\"Competitors Analyzed: {len(results['competitors'])}\")\n",
        "    print(f\"Database Store ID: {results['original_store_id']}\")\n",
        "\n",
        "    # Show database summary\n",
        "    print(f\"\\nðŸ’¾ DATABASE SUMMARY:\")\n",
        "    db_summary = analyzer.get_database_summary()\n",
        "    print(db_summary.to_string(index=False))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Execute the complete demo\n",
        "print(\"ðŸš€ STARTING COMPLETE ANALYSIS...\")\n",
        "demo_results = await run_complete_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up-pOFm3kJRE",
        "outputId": "5dd547e0-2422-4795-b83b-a58d1c5e0094"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ STARTING COMPLETE ANALYSIS...\n",
            "ðŸŽ¯ COMPLETE SHOPIFY COMPETITOR ANALYSIS DEMO\n",
            "============================================================\n",
            "âœ… Database initialized: sqlite:///shopify_competitor_insights.db\n",
            "ðŸš€ Starting comprehensive analysis for: https://memy.co.in\n",
            "================================================================================\n",
            "ðŸ“Š Analyzing original store...\n",
            "ðŸ” Analyzing: https://memy.co.in\n",
            "ðŸ“Š Extracting all data points...\n",
            "ðŸ“¦ Fetching product catalog...\n",
            "ðŸ  Identifying hero products...\n",
            "ðŸ“œ Extracting policies...\n",
            "â“ Extracting FAQs...\n",
            "ðŸ“± Finding social handles...\n",
            "ðŸ“ž Extracting contact info...\n",
            "â„¹ï¸ Extracting brand info...\n",
            "ðŸ”— Finding important links...\n",
            "âœ… Found 2 emails, 3 phones\n",
            "âœ… Found 2 social handles\n",
            "âœ… Found 4 important links\n",
            "âœ… Found brand information\n",
            "âœ… Found 222 products\n",
            "âš ï¸ No FAQs found\n",
            "âœ… Found 2 policies\n",
            "âœ… Found 5 hero products\n",
            "âœ… Store analysis completed!\n",
            "ðŸ” Extracting brand information...\n",
            "ðŸ’¾ Saving original store to database...\n",
            "âœ… Store saved to database with ID: 1\n",
            "ðŸŽ¯ Discovering competitors...\n",
            "Found 2 unique competitors\n",
            "ðŸ“ˆ Analyzing competitor stores...\n",
            "  [1/2] Analyzing: Hair Originals\n",
            "ðŸ” Analyzing: https://hairoriginals.com\n",
            "ðŸ“Š Extracting all data points...\n",
            "ðŸ“¦ Fetching product catalog...\n",
            "ðŸ  Identifying hero products...\n",
            "ðŸ“œ Extracting policies...\n",
            "â“ Extracting FAQs...\n",
            "ðŸ“± Finding social handles...\n",
            "ðŸ“ž Extracting contact info...\n",
            "â„¹ï¸ Extracting brand info...\n",
            "ðŸ”— Finding important links...\n",
            "âœ… Found 1 emails, 3 phones\n",
            "âœ… Found 4 important links\n",
            "âœ… Found 3 social handles\n",
            "âœ… Found 99 products\n",
            "âœ… Found 21 FAQs\n",
            "âš ï¸ No brand info found\n",
            "âœ… Found 1 policies\n",
            "âœ… Found 0 hero products\n",
            "âœ… Store analysis completed!\n",
            "âœ… Store saved to database with ID: 2\n",
            "âœ… Competitor relationship created: 1 -> 2\n",
            "  [2/2] Analyzing: Bewakoof\n",
            "ðŸ” Analyzing: https://www.bewakoof.com\n",
            "ðŸ“Š Extracting all data points...\n",
            "ðŸ“¦ Fetching product catalog...\n",
            "ðŸ  Identifying hero products...\n",
            "ðŸ“œ Extracting policies...\n",
            "â“ Extracting FAQs...\n",
            "ðŸ“± Finding social handles...\n",
            "ðŸ“ž Extracting contact info...\n",
            "â„¹ï¸ Extracting brand info...\n",
            "ðŸ”— Finding important links...\n",
            "âœ… Found 0 social handles\n",
            "âœ… Found 0 hero products\n",
            "âœ… Found 0 products\n",
            "âœ… Found 0 important links\n",
            "âš ï¸ No brand info found\n",
            "âœ… Found 0 emails, 3 phones\n",
            "âš ï¸ No FAQs found\n",
            "âœ… Found 0 policies\n",
            "âœ… Store analysis completed!\n",
            "âœ… Store saved to database with ID: 3\n",
            "âœ… Competitor relationship created: 1 -> 3\n",
            "ðŸ“‹ Generating comprehensive report...\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š COMPREHENSIVE ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "ðŸª ORIGINAL STORE: Memy\n",
            "URL: https://memy.co.in\n",
            "Products: 222\n",
            "Hero Products: 5\n",
            "Social Platforms: 2\n",
            "FAQs: 0\n",
            "Contact Emails: 2\n",
            "Contact Phones: 3\n",
            "\n",
            "ðŸ† COMPETITORS ANALYZED: 2\n",
            "  1. Hairoriginals\n",
            "     URL: https://hairoriginals.com\n",
            "     Products: 99\n",
            "     Discovery Method: industry_analysis\n",
            "  2. Bewakoof\n",
            "     URL: https://www.bewakoof.com\n",
            "     Products: 0\n",
            "     Discovery Method: industry_analysis\n",
            "\n",
            "ðŸ” COMPETITIVE INSIGHTS:\n",
            "Product Catalog Position: above_average\n",
            "Pricing Position: competitive\n",
            "Average Price: $881.85\n",
            "Competitor Average Price: $3023.32\n",
            "\n",
            "ðŸ“¦ PRODUCT INSIGHTS:\n",
            "Top Products:\n",
            "  1. Ashveil Grey Cotton Co-ord Set - $999.0\n",
            "  2. Blushrift Pink Cotton Co-ord Set - $999.0\n",
            "  3. Saffronglow Mustard Cotton Co-ord Set - $999.0\n",
            "  4. Terracrest Rust Cotton Co-ord Set - $999.0\n",
            "  5. Deepwater Blue Cotton Co-ord Set - $999.0\n",
            "\n",
            "ðŸ“± SOCIAL MEDIA PRESENCE:\n",
            "  Facebook: https://www.facebook.com/memy.co.in\n",
            "  Instagram: https://www.instagram.com/memy.co.in/\n",
            "\n",
            "â“ CUSTOMER SUPPORT:\n",
            "\n",
            "ðŸ”— IMPORTANT LINKS:\n",
            "  Shipping: https://memy.co.in/policies/shipping-policy\n",
            "  Contact Us: https://memy.co.in/pages/contact\n",
            "  Size Guide: https://memy.co.in/pages/size-guide\n",
            "  Order Tracking: https://memy.shiprocket.co/tracking\n",
            "\n",
            "================================================================================\n",
            "ðŸ“ Analysis exported to: memy_analysis_20250718_080904.json\n",
            "ðŸ“ Competitive report exported to: memy_competitive_report_20250718_080904.json\n",
            "âœ… Comprehensive analysis completed!\n",
            "\n",
            "ðŸ“Š ANALYSIS SUMMARY:\n",
            "Original Store: Memy\n",
            "Competitors Analyzed: 2\n",
            "Database Store ID: 1\n",
            "\n",
            "ðŸ’¾ DATABASE SUMMARY:\n",
            " Store ID    Store Name Industry  Products  Social Platforms  FAQs  Is Competitor       Scraped At\n",
            "        1          Memy  Apparel       222                 2     0          False 2025-07-18 08:08\n",
            "        2 Hairoriginals  Apparel        99                 3    20           True 2025-07-18 08:08\n",
            "        3      Bewakoof  Apparel         0                 0     0           True 2025-07-18 08:08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ADDITIONAL UTILITIES AND QUERY EXAMPLES\n",
        "# ============================================================================\n",
        "\n",
        "# Quick analysis function\n",
        "async def quick_analysis(store_url: str):\n",
        "    \"\"\"Quick analysis for testing\"\"\"\n",
        "    print(f\"âš¡ Quick Analysis: {store_url}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    analyzer = CompleteShopifyAnalyzer()\n",
        "\n",
        "    # Analyze without competitors for speed\n",
        "    results = await analyzer.comprehensive_analysis(\n",
        "        store_url=store_url,\n",
        "        include_competitors=False\n",
        "    )\n",
        "\n",
        "    insights = results['original_store']\n",
        "    print(f\"ðŸª Store: {insights.store_name}\")\n",
        "    print(f\"ðŸ“¦ Products: {len(insights.product_catalog)}\")\n",
        "    print(f\"ðŸ“± Social: {len(insights.social_handles)}\")\n",
        "    print(f\"â“ FAQs: {len(insights.faqs)}\")\n",
        "    print(f\"ðŸ“ž Contact: {len(insights.contact_info.emails)} emails\")\n",
        "    print(f\"ðŸ”— Links: {len(insights.important_links)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Database query examples\n",
        "def show_database_stats():\n",
        "    \"\"\"Show database statistics\"\"\"\n",
        "    db_manager = DatabaseManager()\n",
        "\n",
        "    print(\"\\nðŸ“Š DATABASE STATISTICS:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Get summary\n",
        "    summary = db_manager.get_store_summary()\n",
        "    print(f\"Total stores in database: {len(summary)}\")\n",
        "    print(f\"Total products: {summary['Products'].sum()}\")\n",
        "    print(f\"Total social handles: {summary['Social Platforms'].sum()}\")\n",
        "    print(f\"Total FAQs: {summary['FAQs'].sum()}\")\n",
        "\n",
        "    print(\"\\nStore Summary:\")\n",
        "    print(summary.to_string(index=False))\n",
        "\n",
        "# File management utilities\n",
        "import os\n",
        "\n",
        "def list_generated_files():\n",
        "    \"\"\"List all generated files\"\"\"\n",
        "    print(\"\\nðŸ“ GENERATED FILES:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    json_files = [f for f in os.listdir('.') if f.endswith('.json')]\n",
        "    db_files = [f for f in os.listdir('.') if f.endswith('.db')]\n",
        "\n",
        "    print(\"JSON Files:\")\n",
        "    for file in json_files:\n",
        "        size = os.path.getsize(file) / 1024\n",
        "        print(f\"  {file} ({size:.1f} KB)\")\n",
        "\n",
        "    print(\"\\nDatabase Files:\")\n",
        "    for file in db_files:\n",
        "        size = os.path.getsize(file) / 1024\n",
        "        print(f\"  {file} ({size:.1f} KB)\")\n",
        "\n",
        "# Show final statistics\n",
        "show_database_stats()\n",
        "list_generated_files()\n",
        "\n",
        "print(\"\\nâœ… Complete Shopify Competitor Analysis System Ready!\")\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸŽ¯ Features Available:\")\n",
        "print(\"âœ… Complete product catalog extraction\")\n",
        "print(\"âœ… Hero products identification\")\n",
        "print(\"âœ… Policy extraction (Privacy, Return, Refund)\")\n",
        "print(\"âœ… FAQ extraction with Q&A pairs\")\n",
        "print(\"âœ… Social media handle discovery\")\n",
        "print(\"âœ… Contact information extraction\")\n",
        "print(\"âœ… About brand content extraction\")\n",
        "print(\"âœ… Important links identification\")\n",
        "print(\"âœ… Competitor discovery & analysis\")\n",
        "print(\"âœ… Full SQL database persistence\")\n",
        "print(\"âœ… Competitive analysis reports\")\n",
        "print(\"âœ… Data export capabilities\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E28TTqjikJTT",
        "outputId": "90722c14-499e-4741-ac70-109b293611a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Database initialized: sqlite:///shopify_competitor_insights.db\n",
            "\n",
            "ðŸ“Š DATABASE STATISTICS:\n",
            "========================================\n",
            "Total stores in database: 3\n",
            "Total products: 321\n",
            "Total social handles: 5\n",
            "Total FAQs: 20\n",
            "\n",
            "Store Summary:\n",
            " Store ID    Store Name Industry  Products  Social Platforms  FAQs  Is Competitor       Scraped At\n",
            "        1          Memy  Apparel       222                 2     0          False 2025-07-18 08:08\n",
            "        2 Hairoriginals  Apparel        99                 3    20           True 2025-07-18 08:08\n",
            "        3      Bewakoof  Apparel         0                 0     0           True 2025-07-18 08:08\n",
            "\n",
            "ðŸ“ GENERATED FILES:\n",
            "==============================\n",
            "JSON Files:\n",
            "  memy_analysis_20250718_080904.json (499.1 KB)\n",
            "  memy_competitive_report_20250718_080904.json (1.8 KB)\n",
            "\n",
            "Database Files:\n",
            "  shopify_competitor_insights.db (840.0 KB)\n",
            "\n",
            "âœ… Complete Shopify Competitor Analysis System Ready!\n",
            "============================================================\n",
            "ðŸŽ¯ Features Available:\n",
            "âœ… Complete product catalog extraction\n",
            "âœ… Hero products identification\n",
            "âœ… Policy extraction (Privacy, Return, Refund)\n",
            "âœ… FAQ extraction with Q&A pairs\n",
            "âœ… Social media handle discovery\n",
            "âœ… Contact information extraction\n",
            "âœ… About brand content extraction\n",
            "âœ… Important links identification\n",
            "âœ… Competitor discovery & analysis\n",
            "âœ… Full SQL database persistence\n",
            "âœ… Competitive analysis reports\n",
            "âœ… Data export capabilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1CeRYsWkJZa"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}